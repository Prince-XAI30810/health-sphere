from config.config import Config
from openai import AzureOpenAI, AsyncAzureOpenAI
from adapters.logger import logger
import time
import random
import asyncio

class AzureOpenAIHelper:
    """
    A helper class for interacting with the Azure OpenAI API to perform 
    text generation and embedding operations.

    Attributes:
        azure_endpoint (str): The endpoint URL for Azure OpenAI.
        api_key (str): The API key for Azure OpenAI.
        api_version (str): The API version for Azure OpenAI.
        model (str): The model used for text generation.
        client (AzureOpenAI): The AzureOpenAI client object used for API interaction.
    """
    
    def __init__(self):
        """
        Initializes the AzureOpenAIHelper with the necessary credentials and configuration.
        Sets up the AzureOpenAI client for communication with the Azure OpenAI service.
        """
        self.azure_endpoint = Config.AZURE_OPENAI_ENDPOINT
        self.api_key = Config.AZURE_OPENAI_KEY
        self.api_version = Config.AZURE_OPENAI_VERSION
        self.client = AzureOpenAI(
            azure_endpoint=self.azure_endpoint,
            api_key=self.api_key,
            api_version=self.api_version,
        )
        logger.info("STATUS: AzureOpenAI Object Created Successfully!")

    def get_response(self, prompt, json_mode=False,model=Config.GPT_GENERATION_4O_MINI_MODEL):
        """
        Sends a prompt to the Azure OpenAI service and retrieves the generated response.
        
        Args:
            prompt (str): The input prompt for generating a response.
            json_mode (bool): If True, the response is returned in JSON format. Default is False.

        Returns:
            str: The response generated by Azure OpenAI.
        """
        response = "Azure OpenAI Not Responding"
        for delay_secs in (2**x for x in range(0, 2)):
            try:
                start = time.time()

                # Handle JSON mode
                if json_mode:
                    response = self.client.chat.completions.create(
                            model=model,
                            temperature=0,
                            seed=123,
                            messages=prompt,
                            response_format={"type": "json_object"}
                            # max_tokens=max_token  # Pass max_token here
                        ).choices[0].message.content
                else:
                    response = (
                        self.client.chat.completions.create(
                            model=model,
                            temperature=0,
                            seed=123,
                            messages=prompt
                            # max_tokens=max_token  # Pass max_token here
                        )
                        .choices[0]
                        .message.content
                    )
                end = time.time()
                # logger.info(f"Response received successfully in {end - start:.3f} seconds.")
                return response
            

            except Exception as ex:
                logger.error(f"Azure OpenAI request failed due to {ex}", exc_info=True)
                random_value = random.randint(0, 1000) / 1000.0
                sleep_dur = delay_secs + random_value
                time.sleep(sleep_dur)
                continue

        return response
         
    def generate_embeddings(self, text, model=Config.GPT_EMBEDDING_MODEL):
        """
        Generates embeddings for a given text using the specified model from Azure OpenAI.

        Args:
            text (str): The input text for which embeddings need to be generated.
            model (str): The model used for generating embeddings. Default is configured in the settings.

        Returns:
            list: The generated embeddings.
        """
        
        for delay_secs in (2**x for x in range(0, 2)):
            try:
                # logger.info(f"Attempting to generate embeddings for the provided text using model: {model}")
                
                # Capture start time
                start = time.time()

                # Generate embeddings
                embedding = self.client.embeddings.create(input=[text], model=model).data[0].embedding

                # Capture end time
                end = time.time()

                # Log the time taken for embedding generation
                # logger.info(f"Embeddings generated successfully in {end - start:.3f} seconds.")
                return embedding
            except Exception as ex:
                logger.error(f"Failed to generate embeddings due to {ex}", exc_info=True)
                random_value = random.randint(0, 1000) / 1000.0
                sleep_dur = delay_secs + random_value
                time.sleep(sleep_dur)
                continue


class AsyncAzureOpenAIHelper:
    """
    Unified helper class for Azure OpenAI chat, streaming, and embedding functionalities.
    """

    def __init__(self):
        self.azure_endpoint = Config.AZURE_OPENAI_ENDPOINT
        self.api_key = Config.AZURE_OPENAI_KEY
        self.api_version = Config.AZURE_OPENAI_VERSION

        self.client = AsyncAzureOpenAI(
            azure_endpoint=self.azure_endpoint,
            api_key=self.api_key,
            api_version=self.api_version,
        )
        logger.info("STATUS: AzureOpenAI Client Initialized Successfully!")

    async def get_response(self, prompt, json_mode=False, model=Config.GPT_GENERATION_4O_MINI_MODEL):
        """
        Sends a non-streaming chat request and returns the response content.
        """
        for delay_secs in (2**x for x in range(0, 2)):
            try:
                start = time.time()
                params = {
                    "model": model,
                    "temperature": 0,
                    "seed": 123,
                    "messages": prompt
                }

                if json_mode:
                    params["response_format"] = {"type": "json_object"}

                response = await self.client.chat.completions.create(**params)
                content = response.choices[0].message.content
                end=time.time()
                logger.info(f"Chat response received in {end - start:.3f} seconds.")
                return content

            except Exception as ex:
                logger.error(f"Azure OpenAI chat request failed: {ex}", exc_info=True)
                await asyncio.sleep(delay_secs + random.uniform(0, 1))
        return "Azure OpenAI Not Responding"

    async def stream_chat_response(self, prompt, model=Config.GPT_GENERATION_4O_MINI_MODEL):
        """
        Streams chat responses asynchronously from Azure OpenAI.
        """
        try:
            response = await self.client.chat.completions.create(
                model=model,
                temperature=0,
                messages=prompt,
                stream=True
            )
            async for chunk in response:
                if chunk.choices and chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
        except Exception as e:
            logger.error(f"Streaming chat error: {e}", exc_info=True)

    async def generate_embeddings(self, text, model=Config.GPT_EMBEDDING_MODEL):
        """
        Generates embeddings for input text using Azure OpenAI.
        """
        for delay_secs in (2**x for x in range(0, 2)):
            try:
                logger.info(f"Generating embeddings using model: {model}")
                start = time.time()

                embedding = await self.client.embeddings.create(input=[text], model=model)
                result = embedding.data[0].embedding

                end = time.time()
                logger.info(f"Embeddings generated in {end - start:.3f} seconds.")
                return result
            except Exception as ex:
                logger.error(f"Embedding generation failed: {ex}", exc_info=True)
                await asyncio.sleep(delay_secs + random.uniform(0, 1))
        return None